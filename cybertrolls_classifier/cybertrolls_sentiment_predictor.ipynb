{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CyberTrolls Classifier Prediction app\n",
    "### Author : Evergreen Technologies\n",
    "### This script loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a cybertroll classifier (postive and negative sentiment)\n",
    "### GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#### Cybertrolls dataset can be found at:\n",
    "https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls \n",
    "Columns: \n",
    "Text: Tweet\n",
    "Labels: \n",
    "1 - Tweet is classified as troll \n",
    "0 - Tweet is classified as not a troll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = '/Volumes/My Passport for Mac/data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'cybertrolls_dataset')\n",
    "CYBERTROLLS_FILE_NAME = \"cybertrolls_dataset.csv\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 125000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath):\n",
    "     if os.path.splitext(filepath)[1] != '.csv':\n",
    "          return  # or whatever\n",
    "     seps = [',', ';', '\\t']                    # ',' is default\n",
    "     encodings = [None, 'utf-8', 'ISO-8859-1']  # None is default\n",
    "     for sep in seps:\n",
    "         for encoding in encodings:\n",
    "              try:\n",
    "                  return pd.read_csv(filepath, encoding=encoding, sep=sep)\n",
    "              except Exception:  # should really be more specific \n",
    "                  pass\n",
    "     raise ValueError(\"{!r} is has no encoding in {} or seperator in {}\"\n",
    "                      .format(filepath, encodings, seps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "index_to_label_dict = {}\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "input_df = read_csv(os.path.join(TEXT_DATA_DIR, CYBERTROLLS_FILE_NAME))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are Few Samples in data\n",
      "<bound method NDFrame.head of                                                     text  label\n",
      "0                                 Get fucking real dude.      1\n",
      "1       She is as dirty as they come  and that crook ...      1\n",
      "2       why did you fuck it up. I could do it all day...      1\n",
      "3       Dude they dont finish enclosing the fucking s...      1\n",
      "4       WTF are you talking about Men? No men thats n...      1\n",
      "...                                                  ...    ...\n",
      "19996    I dont. But what is complaining about it goi...      0\n",
      "19997   Bahah  yeah i&;m totally just gonna&; get pis...      0\n",
      "19998       hahahahaha >:) im evil mwahahahahahahahahaha      0\n",
      "19999            What&;s something unique about Ohio? :)      0\n",
      "20000              Who is the biggest gossiper you know?      0\n",
      "\n",
      "[20001 rows x 2 columns]>\n",
      "Here total number of positive, negative and unsupported (neutral) samples\n",
      "        text\n",
      "label       \n",
      "0      12179\n",
      "1       7822\n",
      "Converting pandas dataframe into lists\n",
      "processing row 0\n",
      "processing row 100\n",
      "processing row 200\n",
      "processing row 300\n",
      "processing row 400\n",
      "processing row 500\n",
      "processing row 600\n",
      "processing row 700\n",
      "processing row 800\n",
      "processing row 900\n",
      "processing row 1000\n",
      "processing row 1100\n",
      "processing row 1200\n",
      "processing row 1300\n",
      "processing row 1400\n",
      "processing row 1500\n",
      "processing row 1600\n",
      "processing row 1700\n",
      "processing row 1800\n",
      "processing row 1900\n",
      "processing row 2000\n",
      "processing row 2100\n",
      "processing row 2200\n",
      "processing row 2300\n",
      "processing row 2400\n",
      "processing row 2500\n",
      "processing row 2600\n",
      "processing row 2700\n",
      "processing row 2800\n",
      "processing row 2900\n",
      "processing row 3000\n",
      "processing row 3100\n",
      "processing row 3200\n",
      "processing row 3300\n",
      "processing row 3400\n",
      "processing row 3500\n",
      "processing row 3600\n",
      "processing row 3700\n",
      "processing row 3800\n",
      "processing row 3900\n",
      "processing row 4000\n",
      "processing row 4100\n",
      "processing row 4200\n",
      "processing row 4300\n",
      "processing row 4400\n",
      "processing row 4500\n",
      "processing row 4600\n",
      "processing row 4700\n",
      "processing row 4800\n",
      "processing row 4900\n",
      "processing row 5000\n",
      "processing row 5100\n",
      "processing row 5200\n",
      "processing row 5300\n",
      "processing row 5400\n",
      "processing row 5500\n",
      "processing row 5600\n",
      "processing row 5700\n",
      "processing row 5800\n",
      "processing row 5900\n",
      "processing row 6000\n",
      "processing row 6100\n",
      "processing row 6200\n",
      "processing row 6300\n",
      "processing row 6400\n",
      "processing row 6500\n",
      "processing row 6600\n",
      "processing row 6700\n",
      "processing row 6800\n",
      "processing row 6900\n",
      "processing row 7000\n",
      "processing row 7100\n",
      "processing row 7200\n",
      "processing row 7300\n",
      "processing row 7400\n",
      "processing row 7500\n",
      "processing row 7600\n",
      "processing row 7700\n",
      "processing row 7800\n",
      "processing row 7900\n",
      "processing row 8000\n",
      "processing row 8100\n",
      "processing row 8200\n",
      "processing row 8300\n",
      "processing row 8400\n",
      "processing row 8500\n",
      "processing row 8600\n",
      "processing row 8700\n",
      "processing row 8800\n",
      "processing row 8900\n",
      "processing row 9000\n",
      "processing row 9100\n",
      "processing row 9200\n",
      "processing row 9300\n",
      "processing row 9400\n",
      "processing row 9500\n",
      "processing row 9600\n",
      "processing row 9700\n",
      "processing row 9800\n",
      "processing row 9900\n",
      "processing row 10000\n",
      "processing row 10100\n",
      "processing row 10200\n",
      "processing row 10300\n",
      "processing row 10400\n",
      "processing row 10500\n",
      "processing row 10600\n",
      "processing row 10700\n",
      "processing row 10800\n",
      "processing row 10900\n",
      "processing row 11000\n",
      "processing row 11100\n",
      "processing row 11200\n",
      "processing row 11300\n",
      "processing row 11400\n",
      "processing row 11500\n",
      "processing row 11600\n",
      "processing row 11700\n",
      "processing row 11800\n",
      "processing row 11900\n",
      "processing row 12000\n",
      "processing row 12100\n",
      "processing row 12200\n",
      "processing row 12300\n",
      "processing row 12400\n",
      "processing row 12500\n",
      "processing row 12600\n",
      "processing row 12700\n",
      "processing row 12800\n",
      "processing row 12900\n",
      "processing row 13000\n",
      "processing row 13100\n",
      "processing row 13200\n",
      "processing row 13300\n",
      "processing row 13400\n",
      "processing row 13500\n",
      "processing row 13600\n",
      "processing row 13700\n",
      "processing row 13800\n",
      "processing row 13900\n",
      "processing row 14000\n",
      "processing row 14100\n",
      "processing row 14200\n",
      "processing row 14300\n",
      "processing row 14400\n",
      "processing row 14500\n",
      "processing row 14600\n",
      "processing row 14700\n",
      "processing row 14800\n",
      "processing row 14900\n",
      "processing row 15000\n",
      "processing row 15100\n",
      "processing row 15200\n",
      "processing row 15300\n",
      "processing row 15400\n",
      "processing row 15500\n",
      "processing row 15600\n",
      "processing row 15700\n",
      "processing row 15800\n",
      "processing row 15900\n",
      "processing row 16000\n",
      "processing row 16100\n",
      "processing row 16200\n",
      "processing row 16300\n",
      "processing row 16400\n",
      "processing row 16500\n",
      "processing row 16600\n",
      "processing row 16700\n",
      "processing row 16800\n",
      "processing row 16900\n",
      "processing row 17000\n",
      "processing row 17100\n",
      "processing row 17200\n",
      "processing row 17300\n",
      "processing row 17400\n",
      "processing row 17500\n",
      "processing row 17600\n",
      "processing row 17700\n",
      "processing row 17800\n",
      "processing row 17900\n",
      "processing row 18000\n",
      "processing row 18100\n",
      "processing row 18200\n",
      "processing row 18300\n",
      "processing row 18400\n",
      "processing row 18500\n",
      "processing row 18600\n",
      "processing row 18700\n",
      "processing row 18800\n",
      "processing row 18900\n",
      "processing row 19000\n",
      "processing row 19100\n",
      "processing row 19200\n",
      "processing row 19300\n",
      "processing row 19400\n",
      "processing row 19500\n",
      "processing row 19600\n",
      "processing row 19700\n",
      "processing row 19800\n",
      "processing row 19900\n",
      "processing row 20000\n",
      "Labels Array\n",
      "20001\n",
      "Labels Dictionary\n",
      "{1: 0, 0: 1}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "ct_df = input_df[['text','label']]\n",
    "print(\"Here are Few Samples in data\")\n",
    "print(ct_df.head)\n",
    "\n",
    "print(\"Here total number of positive, negative and unsupported (neutral) samples\")\n",
    "print(ct_df.groupby(['label']).count())\n",
    "\n",
    "print(\"Converting pandas dataframe into lists\")\n",
    "texts = ct_df['text'].values.tolist()\n",
    "labels = []\n",
    "labels_text = []\n",
    "labels_text_unique = ct_df.label.unique().tolist()\n",
    "labels_text = ct_df['label'].values.tolist()\n",
    "\n",
    "idxCounter = 0\n",
    "for label in labels_text_unique:\n",
    "    labels_index[label] = idxCounter\n",
    "    index_to_label_dict[idxCounter] = label\n",
    "    idxCounter = idxCounter + 1;\n",
    "\n",
    "idxCounter = 0    \n",
    "for label in labels_text:\n",
    "    if idxCounter%100==0:\n",
    "        print(\"processing row \" + str(idxCounter))\n",
    "    labels.append(labels_index[label])\n",
    "    idxCounter = idxCounter + 1;\n",
    "    \n",
    "\n",
    "print(\"Labels Array\")\n",
    "print(len(labels))\n",
    "print(\"Labels Dictionary\")\n",
    "print(labels_index)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model .....\n",
      "Loaded model from disk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model .....\")\n",
    "# load json and create model\n",
    "json_file = open('/Volumes/My Passport for Mac/model/cybertrolls/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/Volumes/My Passport for Mac/model/cybertrolls/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\"I am your worst nightmare. I'll come after you an make your life miserable\" ,\n",
    "              \"You are complete train wreck amigo\",\n",
    "               \"Let’s approach change with an attitude of self-compassion & curiosity as we explore & find our way back to – or into – what works for us & our own, dear, beloved, painful, shining hearts\"\n",
    "             ]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70061845 0.29938164]\n",
      " [0.75756073 0.2424393 ]\n",
      " [0.00156466 0.9984353 ]]\n"
     ]
    }
   ],
   "source": [
    "nn_output = loaded_model.predict(test_data)\n",
    "print(nn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  1\n",
      "text:  I am your worst nightmare. I'll come after you an make your life miserable\n",
      "=====================================\n",
      "Category:  1\n",
      "text:  You are complete train wreck amigo\n",
      "=====================================\n",
      "Category:  0\n",
      "text:  Let’s approach change with an attitude of self-compassion & curiosity as we explore & find our way back to – or into – what works for us & our own, dear, beloved, painful, shining hearts\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for idx in np.argmax(nn_output, axis=1):\n",
    "    print(\"Category: \", index_to_label_dict[idx])\n",
    "    print(\"text: \" , test_texts[i])\n",
    "    print(\"=====================================\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
